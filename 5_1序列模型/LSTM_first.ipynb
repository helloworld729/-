{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    根据图4实现一个LSTM单元的前向传播,是一个时间片上的传播。，可以看做是nx-->na-->ny的网络\n",
    "    \n",
    "    参数：\n",
    "        xt -- 在时间步“t”输入的数据，维度为(n_x, m)\n",
    "        a_prev -- 上一个时间步“t-1”的隐藏状态，维度为(n_a, m)\n",
    "        c_prev -- 上一个时间步“t-1”的记忆状态，维度为(n_a, m)\n",
    "        parameters -- 字典类型的变量，包含了：\n",
    "                        Wf -- 遗忘门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bf -- 遗忘门的偏置，维度为(n_a, 1)\n",
    "                        Wi -- 更新门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bi -- 更新门的偏置，维度为(n_a, 1)\n",
    "                        Wc -- 第一个“tanh”的权值，维度为(n_a, n_a + n_x)\n",
    "                        bc -- 第一个“tanh”的偏置，维度为(n_a, n_a + n_x)\n",
    "                        Wo -- 输出门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bo -- 输出门的偏置，维度为(n_a, 1)\n",
    "                        Wy -- 隐藏状态与输出相关的权值，维度为(n_y, n_a)\n",
    "                        by -- 隐藏状态与输出相关的偏置，维度为(n_y, 1)\n",
    "    返回：\n",
    "        a_next -- 下一个隐藏状态，维度为(n_a, m)\n",
    "        c_next -- 下一个记忆状态，维度为(n_a, m)\n",
    "        yt_pred -- 在时间步“t”的预测，维度为(n_y, m)\n",
    "        cache -- 包含了反向传播所需要的参数，包含了(a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "        \n",
    "    注意：\n",
    "        ft/it/ot表示遗忘/更新/输出门，cct表示候选值(c tilda)，c表示记忆值。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 从“parameters”中获取相关值\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # 获取 xt 与 Wy 的维度信息\n",
    "    n_x, m = xt.shape  # xt的第一个维度是节点数目，第二个维度是样本数目，这与bp网络是一样的\n",
    "    n_y, n_a = Wy.shape  # wy是对a的拟合，由于softmax不改变节点数目，所以wy的第一个维度是输出节点的数目，第二个维度是na，也就是cell数目\n",
    "    \n",
    "    # 1.连接 a_prev 与 xt\n",
    "    contact = np.zeros([n_a + n_x, m])  # 输入与记忆的组合初始化\n",
    "    contact[: n_a, :] = a_prev  # 记忆填充  前na行\n",
    "    contact[n_a :, :] = xt  # 输入填充  na行之后\n",
    "    \n",
    "    # 2.根据公式计算ft、it、cct、c_next、ot、a_next\n",
    "    \n",
    "    ## 遗忘门，公式1\n",
    "    ft = rnn_utils.sigmoid(np.dot(Wf, contact) + bf)\n",
    "    \n",
    "    ## 更新门，公式2\n",
    "    it = rnn_utils.sigmoid(np.dot(Wi, contact) + bi)\n",
    "    \n",
    "    ## 更新单元，公式3\n",
    "    cct = np.tanh(np.dot(Wc, contact) + bc)\n",
    "    \n",
    "    ## 更新单元，公式4\n",
    "    #c_next = np.multiply(ft, c_prev) + np.multiply(it, cct)\n",
    "    c_next = ft * c_prev + it * cct\n",
    "    ## 输出门，公式5\n",
    "    ot = rnn_utils.sigmoid(np.dot(Wo, contact) + bo)\n",
    "    \n",
    "    ## 输出门，公式6\n",
    "    #a_next = np.multiply(ot, np.tan(c_next))\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "    # 3.计算LSTM单元的预测值\n",
    "    yt_pred = rnn_utils.softmax(np.dot(Wy, a_next) + by)\n",
    "    \n",
    "    # 保存包含了反向传播所需要的参数\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "    \n",
    "    return a_next, c_next, yt_pred, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    根据图5来实现LSTM单元组成的的循环神经网络\n",
    "\n",
    "    参数：\n",
    "        x -- 所有时间步的输入数据，维度为(n_x, m, T_x)\n",
    "        a0 -- 初始化隐藏状态，维度为(n_a, m)\n",
    "        parameters -- python字典，包含了以下参数：\n",
    "                        Wf -- 遗忘门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bf -- 遗忘门的偏置，维度为(n_a, 1)\n",
    "                        Wi -- 更新门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bi -- 更新门的偏置，维度为(n_a, 1)\n",
    "                        Wc -- 第一个“tanh”的权值，维度为(n_a, n_a + n_x)\n",
    "                        bc -- 第一个“tanh”的偏置，维度为(n_a, n_a + n_x)\n",
    "                        Wo -- 输出门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bo -- 输出门的偏置，维度为(n_a, 1)\n",
    "                        Wy -- 隐藏状态与输出相关的权值，维度为(n_y, n_a)\n",
    "                        by -- 隐藏状态与输出相关的偏置，维度为(n_y, 1)\n",
    "\n",
    "    返回：\n",
    "        a、c -- 所有时间步的记忆，维度为(n_a, m, T_x)\n",
    "        y -- 所有时间步的预测值，维度为(n_y, m, T_x)\n",
    "        caches -- 为反向传播的保存的元组，维度为（【列表类型】cache, x)）\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化“caches”\n",
    "    caches = []\n",
    "\n",
    "    # 获取 xt 与 Wy 的维度信息\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wy\"].shape\n",
    "\n",
    "    # 使用0来初始化“a”、“c”、“y”\n",
    "    a = np.zeros([n_a, m, T_x])\n",
    "    c = np.zeros([n_a, m, T_x])\n",
    "    y = np.zeros([n_y, m, T_x])\n",
    "\n",
    "    # 初始化“a_next”、“c_next”\n",
    "    a_next = a0\n",
    "    c_next = np.zeros([n_a, m])\n",
    "\n",
    "    # 遍历所有的时间步\n",
    "    for t in range(T_x):\n",
    "        # 更新下一个隐藏状态，下一个记忆状态，计算预测值，获取cache\n",
    "        a_next, c_next, yt_pred, cache = lstm_cell_forward(x[:, :, t], a_next, c_next, parameters)  \n",
    "        \"\"\"\n",
    "        所有样本的第t时间片，可以理解为第t个单词。\n",
    "        c<t>用0初始化， a<t>随机初始化。\n",
    "        parameter也是随机的初始化。\n",
    "        返回 记忆、预测、缓存（输入x、激活值（反向传播）、权重）。\n",
    "        每个时间片更改的信息是：输入、记忆值但是 parameters不会更新。\n",
    "        \"\"\"      \n",
    "\n",
    "        # 保存新的下一个隐藏状态到变量a中\n",
    "        a[:, :, t] = a_next\n",
    "\n",
    "        # 保存预测值到变量y中\n",
    "        y[:, :, t] = yt_pred\n",
    "\n",
    "        # 保存下一个单元状态到变量c中\n",
    "        c[:, :, t] = c_next\n",
    "\n",
    "        # 把cache添加到caches中\n",
    "        caches.append(cache)\n",
    "\n",
    "    # 保存反向传播需要的参数\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches\n",
    "\n",
    "# 执行一轮迭代的前向传播，后面还需要反向传播和更新参数等操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    实现LSTM的单步反向传播\n",
    "    \n",
    "    参数：\n",
    "        da_next -- 下一个隐藏状态的梯度，维度为(n_a, m)\n",
    "        dc_next -- 下一个单元状态的梯度，维度为(n_a, m)\n",
    "        cache -- 来自前向传播的一些参数\n",
    "        \n",
    "    返回：\n",
    "        gradients -- 包含了梯度信息的字典：\n",
    "                        dxt -- 输入数据的梯度，维度为(n_x, m)\n",
    "                        da_prev -- 先前的隐藏状态的梯度，维度为(n_a, m)\n",
    "                        dc_prev -- 前的记忆状态的梯度，维度为(n_a, m, T_x)\n",
    "                        dWf -- 遗忘门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbf -- 遗忘门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWi -- 更新门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbi -- 更新门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWc -- 第一个“tanh”的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbc -- 第一个“tanh”的偏置的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dWo -- 输出门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbo -- 输出门的偏置的梯度，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    # 从cache中获取信息\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
    "    \n",
    "    # 获取xt与a_next的维度信息\n",
    "    n_x, m = xt.shape\n",
    "    n_a, m = a_next.shape\n",
    "    \n",
    "    # 根据公式7-10来计算门的导数\n",
    "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
    "    dcct = (dc_next * it + ot * (1 - np.square(np.tanh(c_next))) * it * da_next) * (1 - np.square(cct))\n",
    "    dit = (dc_next * cct + ot * (1 - np.square(np.tanh(c_next))) * cct * da_next) * it * (1 - it)\n",
    "    dft = (dc_next * c_prev + ot * (1 - np.square(np.tanh(c_next))) * c_prev * da_next) * ft * (1 - ft)\n",
    "    \n",
    "    # 根据公式11-14计算参数的导数\n",
    "    concat = np.concatenate((a_prev, xt), axis=0).T\n",
    "    dWf = np.dot(dft, concat)\n",
    "    dWi = np.dot(dit, concat)\n",
    "    dWc = np.dot(dcct, concat)\n",
    "    dWo = np.dot(dot, concat)\n",
    "    dbf = np.sum(dft,axis=1,keepdims=True)\n",
    "    dbi = np.sum(dit,axis=1,keepdims=True)\n",
    "    dbc = np.sum(dcct,axis=1,keepdims=True)\n",
    "    dbo = np.sum(dot,axis=1,keepdims=True)\n",
    "    \n",
    "    \n",
    "    # 使用公式15-17计算洗起来了隐藏状态、先前记忆状态、输入的导数。\n",
    "    da_prev = np.dot(parameters[\"Wf\"][:, :n_a].T, dft) + np.dot(parameters[\"Wc\"][:, :n_a].T, dcct) +  np.dot(parameters[\"Wi\"][:, :n_a].T, dit) + np.dot(parameters[\"Wo\"][:, :n_a].T, dot)\n",
    "        \n",
    "    dc_prev = dc_next * ft + ot * (1 - np.square(np.tanh(c_next))) * ft * da_next\n",
    "    \n",
    "    dxt = np.dot(parameters[\"Wf\"][:, n_a:].T, dft) + np.dot(parameters[\"Wc\"][:, n_a:].T, dcct) +  np.dot(parameters[\"Wi\"][:, n_a:].T, dit) + np.dot(parameters[\"Wo\"][:, n_a:].T, dot)\n",
    "    \n",
    "    # 保存梯度信息到字典\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    实现LSTM网络的反向传播\n",
    "    \n",
    "    参数：\n",
    "        da -- 关于隐藏状态的梯度，维度为(n_a, m, T_x)\n",
    "        cachses -- 前向传播保存的信息\n",
    "    \n",
    "    返回：\n",
    "        gradients -- 包含了梯度信息的字典：\n",
    "                        dx -- 输入数据的梯度，维度为(n_x, m，T_x)\n",
    "                        da0 -- 先前的隐藏状态的梯度，维度为(n_a, m)\n",
    "                        dWf -- 遗忘门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbf -- 遗忘门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWi -- 更新门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbi -- 更新门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWc -- 第一个“tanh”的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbc -- 第一个“tanh”的偏置的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dWo -- 输出门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbo -- 输出门的偏置的梯度，维度为(n_a, 1)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # 从caches中获取第一个cache（t=1）的值\n",
    "    caches, x = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    # 获取da与x1的维度信息\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # 初始化梯度\n",
    "    dx = np.zeros([n_x, m, T_x])\n",
    "    da0 = np.zeros([n_a, m])\n",
    "    da_prevt = np.zeros([n_a, m])\n",
    "    dc_prevt = np.zeros([n_a, m])\n",
    "    dWf = np.zeros([n_a, n_a + n_x])\n",
    "    dWi = np.zeros([n_a, n_a + n_x])\n",
    "    dWc = np.zeros([n_a, n_a + n_x])\n",
    "    dWo = np.zeros([n_a, n_a + n_x])\n",
    "    dbf = np.zeros([n_a, 1])\n",
    "    dbi = np.zeros([n_a, 1])\n",
    "    dbc = np.zeros([n_a, 1])\n",
    "    dbo = np.zeros([n_a, 1])\n",
    "    \n",
    "    # 处理所有时间步\n",
    "    for t in reversed(range(T_x)):\n",
    "        # 使用lstm_cell_backward函数计算所有梯度\n",
    "        gradients = lstm_cell_backward(da[:,:,t],dc_prevt,caches[t])\n",
    "        # 保存相关参数\n",
    "        dx[:,:,t] = gradients['dxt']\n",
    "        dWf = dWf+gradients['dWf']\n",
    "        dWi = dWi+gradients['dWi']\n",
    "        dWc = dWc+gradients['dWc']\n",
    "        dWo = dWo+gradients['dWo']\n",
    "        dbf = dbf+gradients['dbf']\n",
    "        dbi = dbi+gradients['dbi']\n",
    "        dbc = dbc+gradients['dbc']\n",
    "        dbo = dbo+gradients['dbo']\n",
    "    # 将第一个激活的梯度设置为反向传播的梯度da_prev。\n",
    "    da0 = gradients['da_prev']\n",
    "\n",
    "    # 保存所有梯度到字典变量内\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
